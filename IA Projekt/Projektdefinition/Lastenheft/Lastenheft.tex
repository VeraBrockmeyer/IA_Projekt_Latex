%******************************FORMATIERUNG****************************************
\documentclass[a4paper, 12pt]{article}
\usepackage{scrpage2}
\usepackage{todonotes}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{caption}
\pagestyle{scrheadings}
\clearscrheadfoot
\setheadsepline{.5pt}
\setfootsepline{.5pt}
\automark[section]{chapter}
\ihead{\headmark}
\ohead{\pagemark}
\usepackage[ngerman]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[space]{grffile}
\usepackage{setspace}
\usepackage[T1]{fontenc}
\newcommand{\changefont}[3]{
\fontfamily{#1} \fontseries{#2} \fontshape{#3} \selectfont}
\usepackage{datetime}
\newdateformat{monthyeardate}{%
  \monthname[\THEMONTH] \THEYEAR}
\usepackage{geometry}
\geometry{verbose,a4paper,tmargin=30mm,bmargin=30mm,lmargin=35mm,rmargin=25mm}
\usepackage[numbers,square]{natbib}
\usepackage[
	breaklinks=true,
	pdfauthor={Laura Anger, Vera Brockmeyer, Britta Boerner, Anna Bolder},
	pdftitle={VR_Interface},
	pdftoolbar=true,
	pdfsubject={Projectdefinition},
	colorlinks=true,
	linkcolor=blue,
	citecolor=blue,
	urlcolor=blue,
	linktocpage=true
	]{hyperref}
\usepackage{algorithmicx}
\usepackage{multicol}
\usepackage{multirow}
\usepackage[ruled]{algorithm}
\usepackage{algpseudocode}
\usepackage{pdfpages}
\usepackage[
	font=small,
	labelfont=bf, 
	format=plain,
	indention=1cm
	]{caption}
\setlength{\parindent}{0pt} 
\setlength{\parskip}{.5em}
\usepackage{color}
\definecolor{myColor}{rgb}{0.8,0.8,0.8}
\newcommand{\Absatzbox}[1]{\parbox[0pt][2em][c]{0cm}{}}
\usepackage{listings}

%*****************************ENDE FORMATIERUNG****************************************

\begin{document}
%\linespread{1.2}
%\changefont{ppl}{m}{n}


\input{deckblatt.tex}
    
\section{Project goals}

\begin{itemize}
\item This project aims to implement at least six different options of interactions with an object in virtual reality environment 
\item Two test scenes will be built with Unity
\item System will offer test task as well as measuring and evaluation tools
\item A optional extension of the project with further interactions like walking will be prepared
\item The project will be realised with the HTC Vive
\item The release of the project is planned on July 15th 2017
\end{itemize}

\section{List of requirements}
\subsection{Scene}
At least two scenes will be realised. In the first scene the participant will be able to learn the different options of grabbing. This room is kept simple. The task will be to grab different sizes of cubes from different distances. The participant will be able to change the options of grabbing. She/he should have tested all possible interactions before switching to the other room.

The second room will be modelled after a supermarket. In this room the participant will get different tasks she/he has to complete. These tasks will differ by complexity, distance of grabbing and size of the objects. The participant will be able to change the options of grabbing.

The optional extension of the project will be a start scene. In this scene the user decides which interaction she/he wants to start, for example different ways of moving through a virtual reality scene. Depending on the selected interaction a different scene will be loaded. The implementation of the different interactions -- beside grabbing -- are not part of this project. 

\subsection{Selection}
The interaction possibilities can be categorised into close range and far range. Different close range interactions will be possible, one being that the participant can only grab and move the object if she/he actually touches the object with the controller in the virtual environment. This is a close representation of real world interactions as we can only grab and move objects we can touch. 
The second close range interaction will be that grabbing and moving of objects is possible if the controller is in proximity to the object but not necessarily touching it. The participant does not have to be very accurate when selecting an object. 

The far range interaction will have different options as well. One will be that a beam will shoot out of the controller, extending along the biggest dimension. This means the participant will be able to point at an object he wants to select with the controller. 
Another option will be that the selection beam extends from the participants head making the selection possible by looking at the object. 
A third option could be that the participants arm extends in the direction she/he is pointing with the controller. With that option he can grab objects that are further away. 

All interaction possibilities include grabbing, rotating and positioning the object in a desired area.  

\subsection{Measuring and Validation tools}

 The system offers a three measurements and the related saving of different parameters. First, the accuracy is defined as the distance between the object of interest and the actual selection and will be saved for each try of the user. On the other hand, the duration time is measured for every performed task to compare and validate the perfomance of the different interaction methods. 
 Finally, every single try during a task will be counted and saved to get a conclusion about the learnability, accuracy and perfomance.
 
 Furthermore, there will be a questionnaire designed to give the users of  \textit{Interaction Lab} an usability evaluation tool at hand. This questionnaire will test parameters as tiring, learnability, self-descriptiveness and fulfilling expectations.

\subsection{Hardware}

The implementation of the interactions possibilities only apply to the head-mounted display \textit{HTC Vive} with its controllers. 

\subsection{Dates}
There will be a first prototype in form of a Paper Mockup on the $30^{th}$ of April 2017 and a second prototype on the $31^{st}$ of May. 
The final version of the \textit{Interaction Lab} will be ready by the $1^{st}$ of July. The deadline for this project is the $15^{th}$ of July 2017.
\end{document}